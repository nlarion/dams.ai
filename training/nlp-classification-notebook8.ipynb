# Test loading the user-provided vocabulary file
# Try different potential file names
user_vocab_files = [
    "vocabulary (copy).json",
    "vocabulary.json", 
    "vocab.json"
]

# Find the first file that exists
user_vocab_file = None
for file_path in user_vocab_files:
    if os.path.exists(file_path):
        user_vocab_file = file_path
        break

# Check if we found a vocabulary file
if user_vocab_file:
    print(f"\nTesting with user-provided vocabulary file: {user_vocab_file}")
    
    # Load the tokenizer with the user's vocabulary
    user_tokenizer = CustomTokenizer(user_vocab_file)
    
    # Print information about the loaded vocabulary
    print(f"Loaded vocabulary has {user_tokenizer.num_words} tokens")
    print(f"Special tokens: PAD={user_tokenizer.pad_token}, OOV={user_tokenizer.oov_token}")
    print(f"Case sensitive: {user_tokenizer.case_sensitive}")
    print(f"Preserve punctuation: {user_tokenizer.preserve_punctuation}")
    
    # Print some of the tokens
    print("\nSample tokens from vocabulary:")
    sample_tokens = list(user_tokenizer.word_index.items())[:10]
    for token, index in sample_tokens:
        print(f"  {token}: {index}")
    
    # Test tokenizing a simple text
    test_text = "This is a test with some punctuation: ; . / - \"quoted text\""
    
    # Tokenize without adding special tokens
    tokenized = user_tokenizer.texts_to_sequences([test_text])[0]
    print(f"\nTokenized text: {tokenized}")
    print(f"Decoded back: {user_tokenizer.sequences_to_texts([tokenized])[0]}")
else:
    print(f"No user vocabulary file found in the current directory."){
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Classification Model using TensorFlow\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a vocabulary from a JSON file\n",
    "2. Create a custom tokenizer using the vocabulary\n",
    "3. Load a corpus from a JSON file\n",
    "4. Preprocess the text data\n",
    "5. Build and train a classification model using TensorFlow\n",
    "6. Save the model for use with TensorFlow.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Tokenizer with JSON Vocabulary

The tokenizer supports multiple vocabulary file formats:

1. **Simple Dictionary Format**: A JSON object mapping tokens to indices
   ```json
   {
     "token1": 0,
     "token2": 1,
     "token3": 2
   }
   ```

2. **Structured Format**: A more complex format with additional metadata
   ```json
   {
     "mode": "word",
     "tokens": [
       ["<PAD>", 0],
       ["<OOV>", 1],
       ["< SOS >", 2],
       ["<EOS>", 3]
     ],
     "merges": [],
     "options": {
       "mode": "word",
       "caseSensitive": false,
       "maxVocabSize": 100000,
       "minFrequency": 2,
       "specialTokens": ["<PAD>", "<OOV>", "< SOS >", "<EOS>"]
     }
   }
   ```

The tokenizer can load, use, and save vocabularies in either format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab_file=None):\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.num_words = 0\n",
    "        self.oov_token = '<OOV>'  # Out of vocabulary token\n",
    "        self.pad_token = '<PAD>'\n",
    "        \n",
    "        # If vocab file is provided, load it\n",
    "        if vocab_file:\n",
    "            self.load_vocabulary(vocab_file)\n",
    "        else:\n",
    "            # Initialize with special tokens\n",
    "            self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "            self.index_word = {0: self.pad_token, 1: self.oov_token}\n",
    "            self.num_words = 2\n",
    "    \n",
    "    def load_vocabulary(self, vocab_file):\n",
    "        \"\"\"Load vocabulary from a JSON file\"\"\"\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            vocab_data = json.load(f)\n",
    "            \n",
    "        # Check if the vocabulary is in word->index format or a list of words\n",
    "        if isinstance(vocab_data, dict):\n",
    "            self.word_index = vocab_data\n",
    "            # Make sure special tokens are included\n",
    "            if self.pad_token not in self.word_index:\n",
    "                self.word_index[self.pad_token] = 0\n",
    "            if self.oov_token not in self.word_index:\n",
    "                self.word_index[self.oov_token] = 1\n",
    "        elif isinstance(vocab_data, list):\n",
    "            # Initialize with special tokens\n",
    "            self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "            # Add words from list\n",
    "            for i, word in enumerate(vocab_data):\n",
    "                self.word_index[word] = i + 2  # +2 for special tokens\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
    "        self.num_words = len(self.word_index)\n",
    "        \n",
    "        print(f\"Loaded vocabulary with {self.num_words} words\")\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"Create vocabulary from texts\"\"\"\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            for word in self._text_to_word_sequence(text):\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "        \n",
    "        # Sort words by frequency (most common first)\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        for i, (word, _) in enumerate(sorted_words):\n",
    "            self.word_index[word] = i + 2  # +2 for special tokens\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
    "        self.num_words = len(self.word_index)\n",
    "        \n",
    "        print(f\"Created vocabulary with {self.num_words} words\")\n",
    "    \n",
    "    def _text_to_word_sequence(self, text):\n",
    "        \"\"\"Convert text to lowercase and split into words\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Split into words\n",
    "        return text.split()\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Convert texts to sequences of word indices\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self._text_to_word_sequence(text)\n",
    "            sequence = []\n",
    "            \n",
    "            for word in words:\n",
    "                # Use word index if in vocabulary, otherwise use OOV token\n",
    "                if word in self.word_index:\n",
    "                    sequence.append(self.word_index[word])\n",
    "                else:\n",
    "                    sequence.append(self.word_index[self.oov_token])\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        \"\"\"Convert sequences of word indices to texts\"\"\"\n",
    "        texts = []\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            words = []\n",
    "            \n",
    "            for idx in sequence:\n",
    "                if idx in self.index_word:\n",
    "                    words.append(self.index_word[idx])\n",
    "                else:\n",
    "                    words.append(self.oov_token)\n",
    "            \n",
    "            texts.append(' '.join(words))\n",
    "        \n",
    "        return texts\n",
    "    \n",
    "    def save_vocabulary(self, filepath):\n",
    "        \"\"\"Save vocabulary to a JSON file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.word_index, f)\n",
    "        \n",
    "        print(f\"Saved vocabulary to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Corpus from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corpus_file):\n",
    "    \"\"\"Load corpus from a JSON file\"\"\"\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        corpus_data = json.load(f)\n",
    "    \n",
    "    # Expected format: List of dictionaries with 'text' and 'label' keys\n",
    "    # Or a dictionary with 'texts' and 'labels' keys\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    if isinstance(corpus_data, list):\n",
    "        for item in corpus_data:\n",
    "            texts.append(item.get('text', ''))\n",
    "            labels.append(1 if item.get('label', '').lower() == 'ads' else 0)\n",
    "    elif isinstance(corpus_data, dict):\n",
    "        texts = corpus_data.get('texts', [])\n",
    "        raw_labels = corpus_data.get('labels', [])\n",
    "        labels = [1 if label.lower() == 'ads' else 0 for label in raw_labels]\n",
    "    \n",
    "    print(f\"Loaded corpus with {len(texts)} samples\")\n",
    "    print(f\"Label distribution: {sum(labels)} ads, {len(labels) - sum(labels)} not ads\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# For demonstration purposes, let's create a sample corpus if not available\n",
    "def create_sample_corpus(filepath, num_samples=1000):\n",
    "    \"\"\"Create a sample corpus for demonstration\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Corpus file {filepath} already exists. Skipping creation.\")\n",
    "        return\n",
    "    \n",
    "    # Sample ad texts\n",
    "    ad_prefixes = [\n",
    "        \"Buy now\", \"Limited offer\", \"Discount\", \"Sale\", \"Free shipping\",\n",
    "        \"Best deal\", \"Don't miss\", \"Special price\", \"Act now\", \"New arrival\"\n",
    "    ]\n",
    "    \n",
    "    ad_products = [\n",
    "        \"shoes\", \"smartphone\", \"laptop\", \"clothes\", \"watch\",\n",
    "        \"headphones\", \"camera\", \"TV\", \"furniture\", \"kitchen appliances\"\n",
    "    ]\n",
    "    \n",
    "    ad_suffixes = [\n",
    "        \"at our store\", \"online\", \"with free delivery\", \"today only\",\n",
    "        \"while supplies last\", \"for a limited time\", \"with 50% off\",\n",
    "        \"and get a free gift\", \"before they're gone\", \"and save money\"\n",
    "    ]\n",
    "    \n",
    "    # Sample non-ad texts\n",
    "    non_ad_prefixes = [\n",
    "        \"I think\", \"Today I\", \"The weather is\", \"My friend\", \"Yesterday\",\n",
    "        \"The movie was\", \"I read\", \"Did you know\", \"I'm planning\", \"I heard\"\n",
    "    ]\n",
    "    \n",
    "    non_ad_topics = [\n",
    "        \"went to the park\", \"watched a movie\", \"read a book\", \"cooked dinner\",\n",
    "        \"visited my family\", \"learned something new\", \"had a great day\",\n",
    "        \"started a new hobby\", \"met an old friend\", \"worked on a project\"\n",
    "    ]\n",
    "    \n",
    "    non_ad_suffixes = [\n",
    "        \"and enjoyed it\", \"for the first time\", \"with my friends\", \"yesterday\",\n",
    "        \"last weekend\", \"after work\", \"during my vacation\", \"in the morning\",\n",
    "        \"before going to sleep\", \"and it was fun\"\n",
    "    ]\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # Generate ad samples (roughly half)\n",
    "    for _ in range(num_samples // 2):\n",
    "        prefix = np.random.choice(ad_prefixes)\n",
    "        product = np.random.choice(ad_products)\n",
    "        suffix = np.random.choice(ad_suffixes)\n",
    "        text = f\"{prefix} {product} {suffix}\"\n",
    "        corpus.append({\"text\": text, \"label\": \"ads\"})\n",
    "    \n",
    "    # Generate non-ad samples\n",
    "    for _ in range(num_samples - len(corpus)):\n",
    "        prefix = np.random.choice(non_ad_prefixes)\n",
    "        topic = np.random.choice(non_ad_topics)\n",
    "        suffix = np.random.choice(non_ad_suffixes)\n",
    "        text = f\"{prefix} {topic} {suffix}\"\n",
    "        corpus.append({\"text\": text, \"label\": \"not ads\"})\n",
    "    \n",
    "    # Shuffle the corpus\n",
    "    np.random.shuffle(corpus)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(corpus, f)\n",
    "    \n",
    "    print(f\"Created sample corpus with {len(corpus)} items and saved to {filepath}\")\n",
    "\n",
    "# For demonstration purposes, let's create a sample vocabulary if not available\n",
    "def create_sample_vocabulary(filepath, corpus_texts):\n",
    "    \"\"\"Create a sample vocabulary for demonstration\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Vocabulary file {filepath} already exists. Skipping creation.\")\n",
    "        return\n",
    "    \n",
    "    # Create a tokenizer and fit on texts\n",
    "    tokenizer = CustomTokenizer()\n",
    "    tokenizer.fit_on_texts(corpus_texts)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    tokenizer.save_vocabulary(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample files for demonstration\n",
    "sample_corpus_file = \"sample_corpus.json\"\n",
    "sample_vocab_file = \"sample_vocab.json\"\n",
    "\n",
    "create_sample_corpus(sample_corpus_file)\n",
    "\n",
    "# Load the corpus\n",
    "texts, labels = load_corpus(sample_corpus_file)\n",
    "\n",
    "# Create sample vocabulary file\n",
    "create_sample_vocabulary(sample_vocab_file, texts)\n",
    "\n",
    "# Load the vocabulary and create a tokenizer\n",
    "tokenizer = CustomTokenizer(sample_vocab_file)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Check sequence length distribution\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "plt.hist(sequence_lengths, bins=20)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.show()\n",
    "\n",
    "# Determine maximum sequence length (padding/truncating)\n",
    "max_length = min(max(sequence_lengths), 50)  # Limit to 50 tokens max\n",
    "print(f\"Using maximum sequence length of {max_length}\")\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=max_length,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=tokenizer.word_index[tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "# First split into train+val and test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Then split train+val into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim=16, max_length=50):\n",
    "    \"\"\"Build a text classification model\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(vocab_size=tokenizer.num_words, max_length=max_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Ads', 'Ads']))\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Not Ads', 'Ads'])\n",
    "plt.yticks([0, 1], ['Not Ads', 'Ads'])\n",
    "\n",
    "# Add labels to the plot\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model on New Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, tokenizer, text, max_length):\n",
    "    \"\"\"Predict class for a single text\"\"\"\n",
    "    # Tokenize text\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=max_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=tokenizer.word_index[tokenizer.pad_token]\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    \n",
    "    # Return result\n",
    "    is_ad = prediction > 0.5\n",
    "    label = 'Ad' if is_ad else 'Not Ad'\n",
    "    confidence = prediction if is_ad else 1 - prediction\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'label': label,\n",
    "        'confidence': float(confidence),\n",
    "        'raw_prediction': float(prediction)\n",
    "    }\n",
    "\n",
    "# Test on some new examples\n",
    "test_texts = [\n",
    "    \"Buy our new shoes with 30% discount today!\",\n",
    "    \"The weather is nice today, I'm going for a walk\",\n",
    "    \"Limited offer on all electronics this weekend\",\n",
    "    \"I watched a great movie yesterday with my friends\",\n",
    "    \"This is the best price you will find anywhere\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_text(model, tokenizer, text, max_length)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model for TensorFlow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the tensorflowjs package installed\n",
    "# !pip install tensorflowjs\n",
    "\n",
    "# Save the model in Keras format first\n",
    "model_save_path = \"ad_classification_model\"\n",
    "model.save(model_save_path)\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "# This will create a model.json file and binary weight files\n",
    "tfjs_dir = \"tfjs_model\"\n",
    "!mkdir -p {tfjs_dir}\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, tfjs_dir)\n",
    "\n",
    "print(f\"Model saved in TensorFlow.js format at {tfjs_dir}\")\n",
    "\n",
    "# Save the vocabulary as well\n",
    "tokenizer.save_vocabulary(f\"{tfjs_dir}/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example JavaScript Code to Use the Model in Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tfjs_model/example_usage.js\n",
    "// Example JavaScript code to use the model in a browser\n",
    "// Note: This is just an example and would need to be adapted to your specific use case\n",
    "\n",
    "// Load the model\n",
    "async function loadModel() {\n",
    "  const model = await tf.loadLayersModel('model.json');\n",
    "  return model;\n",
    "}\n",
    "\n",
    "// Load the vocabulary\n",
    "async function loadVocabulary() {\n",
    "  const response = await fetch('vocabulary.json');\n",
    "  const vocab = await response.json();\n",
    "  return vocab;\n",
    "}\n",
    "\n",
    "// Tokenize and pad text\n",
    "function tokenize(text, vocab, maxLength) {\n",
    "  // Convert to lowercase and remove punctuation\n",
    "  const cleanText = text.toLowerCase().replace(/[^\\w\\s]/g, '');\n",
    "  \n",
    "  // Split into words\n",
    "  const words = cleanText.split(/\\s+/);\n",
    "  \n",
    "  // Convert words to indices\n",
    "  const oovIndex = vocab['<OOV>'] || 1;\n",
    "  const sequence = words.map(word => vocab[word] || oovIndex);\n",
    "  \n",
    "  // Truncate or pad as needed\n",
    "  const padIndex = vocab['<PAD>'] || 0;\n",
    "  \n",
    "  if (sequence.length > maxLength) {\n",
    "    return sequence.slice(0, maxLength);\n",
    "  } else {\n",
    "    const padding = Array(maxLength - sequence.length).fill(padIndex);\n",
    "    return [...sequence, ...padding];\n",
    "  }\n",
    "}\n",
    "\n",
    "// Predict class for text\n",
    "async function predictText(text) {\n",
    "  // Load model and vocabulary\n",
    "  const [model, vocab] = await Promise.all([loadModel(), loadVocabulary()]);\n",
    "  \n",
    "  // Define max length\n",
    "  const maxLength = 50;\n",
    "  \n",
    "  // Tokenize text\n",
    "  const sequence = tokenize(text, vocab, maxLength);\n",
    "  \n",
    "  // Create tensor\n",
    "  const inputTensor = tf.tensor2d([sequence], [1, maxLength]);\n",
    "  \n",
    "  // Make prediction\n",
    "  const outputTensor = model.predict(inputTensor);\n",
    "  const prediction = await outputTensor.data();\n",
    "  \n",
    "  // Cleanup\n",
    "  inputTensor.dispose();\n",
    "  outputTensor.dispose();\n",
    "  \n",
    "  // Return result\n",
    "  const isAd = prediction[0] > 0.5;\n",
    "  const label = isAd ? 'Ad' : 'Not Ad';\n",
    "  const confidence = isAd ? prediction[0] : 1 - prediction[0];\n",
    "  \n",
    "  return {\n",
    "    text,\n",
    "    label,\n",
    "    confidence,\n",
    "    rawPrediction: prediction[0]\n",
    "  };\n",
    "}\n",
    "\n",
    "// Example usage\n",
    "async function main() {\n",
    "  const texts = [\n",
    "    \"Buy our new shoes with 30% discount today!\",\n",
    "    \"The weather is nice today, I'm going for a walk\"\n",
    "  ];\n",
    "  \n",
    "  for (const text of texts) {\n",
    "    const result = await predictText(text);\n",
    "    console.log(`Text: ${result.text}`);\n",
    "    console.log(`Prediction: ${result.label} (confidence: ${result.confidence.toFixed(4)})`);\n",
    "    console.log('---');\n",
    "  }\n",
    "}\n",
    "\n",
    "// Call main when document is loaded\n",
    "document.addEventListener('DOMContentLoaded', main);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Created a custom tokenizer that can load vocabulary from a JSON file\n",
    "2. Loaded a corpus from a JSON file (or created a sample one)\n",
    "3. Preprocessed the text data for NLP classification\n",
    "4. Built and trained a TensorFlow model to classify texts as \"ads\" or \"not ads\"\n",
    "5. Evaluated the model's performance\n",
    "6. Saved the model in a format compatible with TensorFlow.js\n",
    "7. Provided example JavaScript code to use the model in a browser\n",
    "\n",
    "The key components that satisfy the requirements are:\n",
    "- Custom tokenizer with JSON vocabulary loading (Section 2)\n",
    "- Corpus loading from JSON (Section 3)\n",
    "- Binary classification for \"ads\" and \"not ads\" (Sections 5-7)\n",
    "- Model export for TensorFlow.js (Section 9)\n",
    "\n",
    "For a real-world application, you may want to consider:\n",
    "- Using a more sophisticated model architecture (e.g., LSTM, Transformer)\n",
    "- Using pre-trained word embeddings\n",
    "- Implementing more advanced text preprocessing\n",
    "- Collecting a larger and more diverse dataset\n",
    "- Adding more classes if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
