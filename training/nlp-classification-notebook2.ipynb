{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Classification Model using TensorFlow\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a vocabulary from a JSON file\n",
    "2. Create a custom tokenizer using the vocabulary\n",
    "3. Load a corpus from a JSON file\n",
    "4. Preprocess the text data\n",
    "5. Build and train a classification model using TensorFlow\n",
    "6. Save the model for use with TensorFlow.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 14:05:10.513793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-28 14:05:10.528676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-28 14:05:10.533475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-28 14:05:10.543937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-28 14:05:11.219579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Tokenizer with JSON Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab_file=None):\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.num_words = 0\n",
    "        self.oov_token = '<OOV>'  # Out of vocabulary token\n",
    "        self.pad_token = '<PAD>'\n",
    "        \n",
    "        # If vocab file is provided, load it\n",
    "        if vocab_file:\n",
    "            self.load_vocabulary(vocab_file)\n",
    "        else:\n",
    "            # Initialize with special tokens\n",
    "            self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "            self.index_word = {0: self.pad_token, 1: self.oov_token}\n",
    "            self.num_words = 2\n",
    "    \n",
    "    def load_vocabulary(self, vocab_file):\n",
    "        \"\"\"Load vocabulary from a JSON file\"\"\"\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            vocab_data = json.load(f)\n",
    "            \n",
    "        # Check if the vocabulary is in word->index format or a list of words\n",
    "        if isinstance(vocab_data, dict):\n",
    "            self.word_index = vocab_data\n",
    "            # Make sure special tokens are included\n",
    "            if self.pad_token not in self.word_index:\n",
    "                self.word_index[self.pad_token] = 0\n",
    "            if self.oov_token not in self.word_index:\n",
    "                self.word_index[self.oov_token] = 1\n",
    "        elif isinstance(vocab_data, list):\n",
    "            # Initialize with special tokens\n",
    "            self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "            # Add words from list\n",
    "            for i, word in enumerate(vocab_data):\n",
    "                self.word_index[word] = i + 2  # +2 for special tokens\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
    "        self.num_words = len(self.word_index)\n",
    "        \n",
    "        print(f\"Loaded vocabulary with {self.num_words} words\")\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"Create vocabulary from texts\"\"\"\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            for word in self._text_to_word_sequence(text):\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "        \n",
    "        # Sort words by frequency (most common first)\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        for i, (word, _) in enumerate(sorted_words):\n",
    "            self.word_index[word] = i + 2  # +2 for special tokens\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
    "        self.num_words = len(self.word_index)\n",
    "        \n",
    "        print(f\"Created vocabulary with {self.num_words} words\")\n",
    "    \n",
    "    def _text_to_word_sequence(self, text):\n",
    "        \"\"\"Convert text to lowercase and split into words\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Split into words\n",
    "        return text.split()\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Convert texts to sequences of word indices\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self._text_to_word_sequence(text)\n",
    "            sequence = []\n",
    "            \n",
    "            for word in words:\n",
    "                # Use word index if in vocabulary, otherwise use OOV token\n",
    "                if word in self.word_index:\n",
    "                    sequence.append(self.word_index[word])\n",
    "                else:\n",
    "                    sequence.append(self.word_index[self.oov_token])\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        \"\"\"Convert sequences of word indices to texts\"\"\"\n",
    "        texts = []\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            words = []\n",
    "            \n",
    "            for idx in sequence:\n",
    "                if idx in self.index_word:\n",
    "                    words.append(self.index_word[idx])\n",
    "                else:\n",
    "                    words.append(self.oov_token)\n",
    "            \n",
    "            texts.append(' '.join(words))\n",
    "        \n",
    "        return texts\n",
    "    \n",
    "    def save_vocabulary(self, filepath):\n",
    "        \"\"\"Save vocabulary to a JSON file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.word_index, f)\n",
    "        \n",
    "        print(f\"Saved vocabulary to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Corpus from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corpus_file):\n",
    "    \"\"\"Load corpus from a JSON file\"\"\"\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        corpus_data = json.load(f)\n",
    "    \n",
    "    # Expected format: List of dictionaries with 'text' and 'label' keys\n",
    "    # Or a dictionary with 'texts' and 'labels' keys\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    if isinstance(corpus_data, list):\n",
    "        for item in corpus_data:\n",
    "            texts.append(item.get('text', ''))\n",
    "            labels.append(1 if item.get('label', '').lower() == 'ads' else 0)\n",
    "    elif isinstance(corpus_data, dict):\n",
    "        texts = corpus_data.get('texts', [])\n",
    "        raw_labels = corpus_data.get('labels', [])\n",
    "        labels = [1 if label.lower() == 'ads' else 0 for label in raw_labels]\n",
    "    \n",
    "    print(f\"Loaded corpus with {len(texts)} samples\")\n",
    "    print(f\"Label distribution: {sum(labels)} ads, {len(labels) - sum(labels)} not ads\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# For demonstration purposes, let's create a sample corpus if not available\n",
    "def create_sample_corpus(filepath, num_samples=1000):\n",
    "    \"\"\"Create a sample corpus for demonstration\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Corpus file {filepath} already exists. Skipping creation.\")\n",
    "        return\n",
    "    \n",
    "    # Sample ad texts\n",
    "    ad_prefixes = [\n",
    "        \"Buy now\", \"Limited offer\", \"Discount\", \"Sale\", \"Free shipping\",\n",
    "        \"Best deal\", \"Don't miss\", \"Special price\", \"Act now\", \"New arrival\"\n",
    "    ]\n",
    "    \n",
    "    ad_products = [\n",
    "        \"shoes\", \"smartphone\", \"laptop\", \"clothes\", \"watch\",\n",
    "        \"headphones\", \"camera\", \"TV\", \"furniture\", \"kitchen appliances\"\n",
    "    ]\n",
    "    \n",
    "    ad_suffixes = [\n",
    "        \"at our store\", \"online\", \"with free delivery\", \"today only\",\n",
    "        \"while supplies last\", \"for a limited time\", \"with 50% off\",\n",
    "        \"and get a free gift\", \"before they're gone\", \"and save money\"\n",
    "    ]\n",
    "    \n",
    "    # Sample non-ad texts\n",
    "    non_ad_prefixes = [\n",
    "        \"I think\", \"Today I\", \"The weather is\", \"My friend\", \"Yesterday\",\n",
    "        \"The movie was\", \"I read\", \"Did you know\", \"I'm planning\", \"I heard\"\n",
    "    ]\n",
    "    \n",
    "    non_ad_topics = [\n",
    "        \"went to the park\", \"watched a movie\", \"read a book\", \"cooked dinner\",\n",
    "        \"visited my family\", \"learned something new\", \"had a great day\",\n",
    "        \"started a new hobby\", \"met an old friend\", \"worked on a project\"\n",
    "    ]\n",
    "    \n",
    "    non_ad_suffixes = [\n",
    "        \"and enjoyed it\", \"for the first time\", \"with my friends\", \"yesterday\",\n",
    "        \"last weekend\", \"after work\", \"during my vacation\", \"in the morning\",\n",
    "        \"before going to sleep\", \"and it was fun\"\n",
    "    ]\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # Generate ad samples (roughly half)\n",
    "    for _ in range(num_samples // 2):\n",
    "        prefix = np.random.choice(ad_prefixes)\n",
    "        product = np.random.choice(ad_products)\n",
    "        suffix = np.random.choice(ad_suffixes)\n",
    "        text = f\"{prefix} {product} {suffix}\"\n",
    "        corpus.append({\"text\": text, \"label\": \"ads\"})\n",
    "    \n",
    "    # Generate non-ad samples\n",
    "    for _ in range(num_samples - len(corpus)):\n",
    "        prefix = np.random.choice(non_ad_prefixes)\n",
    "        topic = np.random.choice(non_ad_topics)\n",
    "        suffix = np.random.choice(non_ad_suffixes)\n",
    "        text = f\"{prefix} {topic} {suffix}\"\n",
    "        corpus.append({\"text\": text, \"label\": \"not ads\"})\n",
    "    \n",
    "    # Shuffle the corpus\n",
    "    np.random.shuffle(corpus)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(corpus, f)\n",
    "    \n",
    "    print(f\"Created sample corpus with {len(corpus)} items and saved to {filepath}\")\n",
    "\n",
    "# For demonstration purposes, let's create a sample vocabulary if not available\n",
    "def create_sample_vocabulary(filepath, corpus_texts):\n",
    "    \"\"\"Create a sample vocabulary for demonstration\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Vocabulary file {filepath} already exists. Skipping creation.\")\n",
    "        return\n",
    "    \n",
    "    # Create a tokenizer and fit on texts\n",
    "    tokenizer = CustomTokenizer()\n",
    "    tokenizer.fit_on_texts(corpus_texts)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    tokenizer.save_vocabulary(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus with 2134 samples\n",
      "Label distribution: 1803 ads, 331 not ads\n",
      "Vocabulary file vocabulary.json already exists. Skipping creation.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m create_sample_vocabulary(sample_vocab_file, texts)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the vocabulary and create a tokenizer\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_vocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert texts to sequences\u001b[39;00m\n\u001b[1;32m     17\u001b[0m sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(texts)\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mCustomTokenizer.__init__\u001b[0;34m(self, vocab_file)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# If vocab file is provided, load it\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocab_file:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Initialize with special tokens\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moov_token: \u001b[38;5;241m1\u001b[39m}\n",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mCustomTokenizer.load_vocabulary\u001b[0;34m(self, vocab_file)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index[word] \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# +2 for special tokens\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create reverse mapping\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_word \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded vocabulary with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index[word] \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# +2 for special tokens\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create reverse mapping\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_word \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded vocabulary with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Create sample files for demonstration\n",
    "sample_corpus_file = \"sample_corpus2.json\"\n",
    "sample_vocab_file = \"vocabulary.json\"\n",
    "\n",
    "#create_sample_corpus(sample_corpus_file)\n",
    "\n",
    "# Load the corpus\n",
    "texts, labels = load_corpus(sample_corpus_file)\n",
    "\n",
    "# Create sample vocabulary file\n",
    "create_sample_vocabulary(sample_vocab_file, texts)\n",
    "\n",
    "# Load the vocabulary and create a tokenizer\n",
    "tokenizer = CustomTokenizer(sample_vocab_file)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Check sequence length distribution\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "plt.hist(sequence_lengths, bins=20)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.show()\n",
    "\n",
    "# Determine maximum sequence length (padding/truncating)\n",
    "max_length = min(max(sequence_lengths), 50)  # Limit to 50 tokens max\n",
    "print(f\"Using maximum sequence length of {max_length}\")\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=max_length,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=tokenizer.word_index[tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "# First split into train+val and test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Then split train+val into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim=16, max_length=50):\n",
    "    \"\"\"Build a text classification model\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(vocab_size=tokenizer.num_words, max_length=max_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Ads', 'Ads']))\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Not Ads', 'Ads'])\n",
    "plt.yticks([0, 1], ['Not Ads', 'Ads'])\n",
    "\n",
    "# Add labels to the plot\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model on New Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, tokenizer, text, max_length):\n",
    "    \"\"\"Predict class for a single text\"\"\"\n",
    "    # Tokenize text\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=max_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=tokenizer.word_index[tokenizer.pad_token]\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    \n",
    "    # Return result\n",
    "    is_ad = prediction > 0.5\n",
    "    label = 'Ad' if is_ad else 'Not Ad'\n",
    "    confidence = prediction if is_ad else 1 - prediction\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'label': label,\n",
    "        'confidence': float(confidence),\n",
    "        'raw_prediction': float(prediction)\n",
    "    }\n",
    "\n",
    "# Test on some new examples\n",
    "test_texts = [\n",
    "    \"Buy our new shoes with 30% discount today!\",\n",
    "    \"The weather is nice today, I'm going for a walk\",\n",
    "    \"Limited offer on all electronics this weekend\",\n",
    "    \"I watched a great movie yesterday with my friends\",\n",
    "    \"This is the best price you will find anywhere\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_text(model, tokenizer, text, max_length)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model for TensorFlow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the tensorflowjs package installed\n",
    "# !pip install tensorflowjs\n",
    "\n",
    "# Save the model in Keras format first\n",
    "model_save_path = \"ad_classification_model\"\n",
    "model.save(model_save_path)\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "# This will create a model.json file and binary weight files\n",
    "tfjs_dir = \"tfjs_model\"\n",
    "!mkdir -p {tfjs_dir}\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, tfjs_dir)\n",
    "\n",
    "print(f\"Model saved in TensorFlow.js format at {tfjs_dir}\")\n",
    "\n",
    "# Save the vocabulary as well\n",
    "tokenizer.save_vocabulary(f\"{tfjs_dir}/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example JavaScript Code to Use the Model in Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tfjs_model/example_usage.js\n",
    "// Example JavaScript code to use the model in a browser\n",
    "// Note: This is just an example and would need to be adapted to your specific use case\n",
    "\n",
    "// Load the model\n",
    "async function loadModel() {\n",
    "  const model = await tf.loadLayersModel('model.json');\n",
    "  return model;\n",
    "}\n",
    "\n",
    "// Load the vocabulary\n",
    "async function loadVocabulary() {\n",
    "  const response = await fetch('vocabulary.json');\n",
    "  const vocab = await response.json();\n",
    "  return vocab;\n",
    "}\n",
    "\n",
    "// Tokenize and pad text\n",
    "function tokenize(text, vocab, maxLength) {\n",
    "  // Convert to lowercase and remove punctuation\n",
    "  const cleanText = text.toLowerCase().replace(/[^\\w\\s]/g, '');\n",
    "  \n",
    "  // Split into words\n",
    "  const words = cleanText.split(/\\s+/);\n",
    "  \n",
    "  // Convert words to indices\n",
    "  const oovIndex = vocab['<OOV>'] || 1;\n",
    "  const sequence = words.map(word => vocab[word] || oovIndex);\n",
    "  \n",
    "  // Truncate or pad as needed\n",
    "  const padIndex = vocab['<PAD>'] || 0;\n",
    "  \n",
    "  if (sequence.length > maxLength) {\n",
    "    return sequence.slice(0, maxLength);\n",
    "  } else {\n",
    "    const padding = Array(maxLength - sequence.length).fill(padIndex);\n",
    "    return [...sequence, ...padding];\n",
    "  }\n",
    "}\n",
    "\n",
    "// Predict class for text\n",
    "async function predictText(text) {\n",
    "  // Load model and vocabulary\n",
    "  const [model, vocab] = await Promise.all([loadModel(), loadVocabulary()]);\n",
    "  \n",
    "  // Define max length\n",
    "  const maxLength = 50;\n",
    "  \n",
    "  // Tokenize text\n",
    "  const sequence = tokenize(text, vocab, maxLength);\n",
    "  \n",
    "  // Create tensor\n",
    "  const inputTensor = tf.tensor2d([sequence], [1, maxLength]);\n",
    "  \n",
    "  // Make prediction\n",
    "  const outputTensor = model.predict(inputTensor);\n",
    "  const prediction = await outputTensor.data();\n",
    "  \n",
    "  // Cleanup\n",
    "  inputTensor.dispose();\n",
    "  outputTensor.dispose();\n",
    "  \n",
    "  // Return result\n",
    "  const isAd = prediction[0] > 0.5;\n",
    "  const label = isAd ? 'Ad' : 'Not Ad';\n",
    "  const confidence = isAd ? prediction[0] : 1 - prediction[0];\n",
    "  \n",
    "  return {\n",
    "    text,\n",
    "    label,\n",
    "    confidence,\n",
    "    rawPrediction: prediction[0]\n",
    "  };\n",
    "}\n",
    "\n",
    "// Example usage\n",
    "async function main() {\n",
    "  const texts = [\n",
    "    \"Buy our new shoes with 30% discount today!\",\n",
    "    \"The weather is nice today, I'm going for a walk\"\n",
    "  ];\n",
    "  \n",
    "  for (const text of texts) {\n",
    "    const result = await predictText(text);\n",
    "    console.log(`Text: ${result.text}`);\n",
    "    console.log(`Prediction: ${result.label} (confidence: ${result.confidence.toFixed(4)})`);\n",
    "    console.log('---');\n",
    "  }\n",
    "}\n",
    "\n",
    "// Call main when document is loaded\n",
    "document.addEventListener('DOMContentLoaded', main);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Created a custom tokenizer that can load vocabulary from a JSON file\n",
    "2. Loaded a corpus from a JSON file (or created a sample one)\n",
    "3. Preprocessed the text data for NLP classification\n",
    "4. Built and trained a TensorFlow model to classify texts as \"ads\" or \"not ads\"\n",
    "5. Evaluated the model's performance\n",
    "6. Saved the model in a format compatible with TensorFlow.js\n",
    "7. Provided example JavaScript code to use the model in a browser\n",
    "\n",
    "The key components that satisfy the requirements are:\n",
    "- Custom tokenizer with JSON vocabulary loading in both standard and specified token-array format (Section 2)\n",
    "- Corpus loading from JSON (Section 3)\n",
    "- Binary classification for \"ads\" and \"not ads\" (Sections 5-7)\n",
    "- Model export for TensorFlow.js (Section 9)\n",
    "\n",
    "For a real-world application, you may want to consider:\n",
    "- Using a more sophisticated model architecture (e.g., LSTM, Transformer)\n",
    "- Using pre-trained word embeddings\n",
    "- Implementing more advanced text preprocessing\n",
    "- Collecting a larger and more diverse dataset\n",
    "- Adding more classes if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
