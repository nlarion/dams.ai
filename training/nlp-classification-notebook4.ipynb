{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Classification Model using TensorFlow\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a vocabulary from a JSON file\n",
    "2. Create a custom tokenizer using the vocabulary\n",
    "3. Load a corpus from a JSON file\n",
    "4. Preprocess the text data\n",
    "5. Build and train a classification model using TensorFlow\n",
    "6. Save the model for use with TensorFlow.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "keras.backend.clear_session()\n",
    "print(\"Keras backend session cleared\")\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Tokenizer with JSON Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab_file=None):\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.num_words = 0\n",
    "        self.oov_token = '<OOV>'  # Out of vocabulary token\n",
    "        self.pad_token = '<PAD>'\n",
    "        \n",
    "        # If vocab file is provided, load it\n",
    "        if vocab_file:\n",
    "            self.load_vocabulary(vocab_file)\n",
    "        else:\n",
    "            # Initialize with special tokens\n",
    "            self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "            self.index_word = {0: self.pad_token, 1: self.oov_token}\n",
    "            self.num_words = 2\n",
    "    \n",
    "    def load_vocabulary(self, vocab_file):\n",
    "        \"\"\"Load vocabulary from a JSON file\"\"\"\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            vocab_data = json.load(f)\n",
    "            \n",
    "        # Check if the vocabulary is in word->index format or a list of words\n",
    "        if isinstance(vocab_data, dict):\n",
    "            self.word_index = vocab_data\n",
    "            # Make sure special tokens are included\n",
    "            if self.pad_token not in self.word_index:\n",
    "                self.word_index[self.pad_token] = 0\n",
    "            if self.oov_token not in self.word_index:\n",
    "                self.word_index[self.oov_token] = 1\n",
    "        elif isinstance(vocab_data, list):\n",
    "            # Initialize with special tokens\n",
    "            self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "            # Add words from list\n",
    "            for i, word in enumerate(vocab_data):\n",
    "                self.word_index[word] = i + 2  # +2 for special tokens\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
    "        self.num_words = len(self.word_index)\n",
    "        \n",
    "        print(f\"Loaded vocabulary with {self.num_words} words\")\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"Create vocabulary from texts\"\"\"\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            for word in self._text_to_word_sequence(text):\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "        \n",
    "        # Sort words by frequency (most common first)\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self.word_index = {self.pad_token: 0, self.oov_token: 1}\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        for i, (word, _) in enumerate(sorted_words):\n",
    "            self.word_index[word] = i + 2  # +2 for special tokens\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
    "        self.num_words = len(self.word_index)\n",
    "        \n",
    "        print(f\"Created vocabulary with {self.num_words} words\")\n",
    "    \n",
    "    def _text_to_word_sequence(self, text):\n",
    "        \"\"\"Convert text to lowercase and split into words\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Split into words\n",
    "        return text.split()\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Convert texts to sequences of word indices\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self._text_to_word_sequence(text)\n",
    "            sequence = []\n",
    "            \n",
    "            for word in words:\n",
    "                # Use word index if in vocabulary, otherwise use OOV token\n",
    "                if word in self.word_index:\n",
    "                    sequence.append(self.word_index[word])\n",
    "                else:\n",
    "                    sequence.append(self.word_index[self.oov_token])\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        \"\"\"Convert sequences of word indices to texts\"\"\"\n",
    "        texts = []\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            words = []\n",
    "            \n",
    "            for idx in sequence:\n",
    "                if idx in self.index_word:\n",
    "                    words.append(self.index_word[idx])\n",
    "                else:\n",
    "                    words.append(self.oov_token)\n",
    "            \n",
    "            texts.append(' '.join(words))\n",
    "        \n",
    "        return texts\n",
    "    \n",
    "    def save_vocabulary(self, filepath):\n",
    "        \"\"\"Save vocabulary to a JSON file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.word_index, f)\n",
    "        \n",
    "        print(f\"Saved vocabulary to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Corpus from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corpus_file):\n",
    "    \"\"\"Load corpus from a JSON file\"\"\"\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        corpus_data = json.load(f)\n",
    "    \n",
    "    # Expected format: List of dictionaries with 'text' and 'label' keys\n",
    "    # Or a dictionary with 'texts' and 'labels' keys\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    if isinstance(corpus_data, list):\n",
    "        for item in corpus_data:\n",
    "            texts.append(item.get('text', ''))\n",
    "            labels.append(1 if item.get('label', '').lower() == 'ads' else 0)\n",
    "    elif isinstance(corpus_data, dict):\n",
    "        texts = corpus_data.get('texts', [])\n",
    "        raw_labels = corpus_data.get('labels', [])\n",
    "        labels = [1 if label.lower() == 'ads' else 0 for label in raw_labels]\n",
    "    \n",
    "    print(f\"Loaded corpus with {len(texts)} samples\")\n",
    "    print(f\"Label distribution: {sum(labels)} ads, {len(labels) - sum(labels)} not ads\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# For demonstration purposes, let's create a sample corpus if not available\n",
    "def create_sample_corpus(filepath, num_samples=1000):\n",
    "    \"\"\"Create a sample corpus for demonstration\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Corpus file {filepath} already exists. Skipping creation.\")\n",
    "        return\n",
    "    \n",
    "    # Sample ad texts\n",
    "    ad_prefixes = [\n",
    "        \"Buy now\", \"Limited offer\", \"Discount\", \"Sale\", \"Free shipping\",\n",
    "        \"Best deal\", \"Don't miss\", \"Special price\", \"Act now\", \"New arrival\"\n",
    "    ]\n",
    "    \n",
    "    ad_products = [\n",
    "        \"shoes\", \"smartphone\", \"laptop\", \"clothes\", \"watch\",\n",
    "        \"headphones\", \"camera\", \"TV\", \"furniture\", \"kitchen appliances\"\n",
    "    ]\n",
    "    \n",
    "    ad_suffixes = [\n",
    "        \"at our store\", \"online\", \"with free delivery\", \"today only\",\n",
    "        \"while supplies last\", \"for a limited time\", \"with 50% off\",\n",
    "        \"and get a free gift\", \"before they're gone\", \"and save money\"\n",
    "    ]\n",
    "    \n",
    "    # Sample non-ad texts\n",
    "    non_ad_prefixes = [\n",
    "        \"I think\", \"Today I\", \"The weather is\", \"My friend\", \"Yesterday\",\n",
    "        \"The movie was\", \"I read\", \"Did you know\", \"I'm planning\", \"I heard\"\n",
    "    ]\n",
    "    \n",
    "    non_ad_topics = [\n",
    "        \"went to the park\", \"watched a movie\", \"read a book\", \"cooked dinner\",\n",
    "        \"visited my family\", \"learned something new\", \"had a great day\",\n",
    "        \"started a new hobby\", \"met an old friend\", \"worked on a project\"\n",
    "    ]\n",
    "    \n",
    "    non_ad_suffixes = [\n",
    "        \"and enjoyed it\", \"for the first time\", \"with my friends\", \"yesterday\",\n",
    "        \"last weekend\", \"after work\", \"during my vacation\", \"in the morning\",\n",
    "        \"before going to sleep\", \"and it was fun\"\n",
    "    ]\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # Generate ad samples (roughly half)\n",
    "    for _ in range(num_samples // 2):\n",
    "        prefix = np.random.choice(ad_prefixes)\n",
    "        product = np.random.choice(ad_products)\n",
    "        suffix = np.random.choice(ad_suffixes)\n",
    "        text = f\"{prefix} {product} {suffix}\"\n",
    "        corpus.append({\"text\": text, \"label\": \"ads\"})\n",
    "    \n",
    "    # Generate non-ad samples\n",
    "    for _ in range(num_samples - len(corpus)):\n",
    "        prefix = np.random.choice(non_ad_prefixes)\n",
    "        topic = np.random.choice(non_ad_topics)\n",
    "        suffix = np.random.choice(non_ad_suffixes)\n",
    "        text = f\"{prefix} {topic} {suffix}\"\n",
    "        corpus.append({\"text\": text, \"label\": \"not ads\"})\n",
    "    \n",
    "    # Shuffle the corpus\n",
    "    np.random.shuffle(corpus)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(corpus, f)\n",
    "    \n",
    "    print(f\"Created sample corpus with {len(corpus)} items and saved to {filepath}\")\n",
    "\n",
    "# For demonstration purposes, let's create a sample vocabulary if not available\n",
    "def create_sample_vocabulary(filepath, corpus_texts):\n",
    "    \"\"\"Create a sample vocabulary for demonstration\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Vocabulary file {filepath} already exists. Skipping creation.\")\n",
    "        return\n",
    "    \n",
    "    # Create a tokenizer and fit on texts\n",
    "    tokenizer = CustomTokenizer()\n",
    "    tokenizer.fit_on_texts(corpus_texts)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    tokenizer.save_vocabulary(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus file sample_corpus.json already exists. Skipping creation.\n",
      "Loaded corpus with 1000 samples\n",
      "Label distribution: 500 ads, 500 not ads\n",
      "Vocabulary file sample_vocab.json already exists. Skipping creation.\n",
      "Loaded vocabulary with 107 words\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using maximum sequence length of 11\n",
      "Training set: 600 samples\n",
      "Validation set: 200 samples\n",
      "Test set: 200 samples\n"
     ]
    }
   ],
   "source": [
    "# Create sample files for demonstration\n",
    "sample_corpus_file = \"sample_corpus.json\"\n",
    "sample_vocab_file = \"sample_vocab.json\"\n",
    "\n",
    "create_sample_corpus(sample_corpus_file)\n",
    "\n",
    "# Load the corpus\n",
    "texts, labels = load_corpus(sample_corpus_file)\n",
    "\n",
    "# Create sample vocabulary file\n",
    "create_sample_vocabulary(sample_vocab_file, texts)\n",
    "\n",
    "# Load the vocabulary and create a tokenizer\n",
    "tokenizer = CustomTokenizer(sample_vocab_file)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Check sequence length distribution\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "plt.hist(sequence_lengths, bins=20)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.show()\n",
    "\n",
    "# Determine maximum sequence length (padding/truncating)\n",
    "max_length = min(max(sequence_lengths), 50)  # Limit to 50 tokens max\n",
    "print(f\"Using maximum sequence length of {max_length}\")\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=max_length,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=tokenizer.word_index[tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "# First split into train+val and test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Then split train+val into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer num_words: 107\n",
      "Word index size: 107\n",
      "First few words in vocab: [('<PAD>', 0), ('<OOV>', 1), ('a', 2), ('the', 3), ('and', 4)]\n",
      "Creating model with vocab_size: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743119376.119651   49172 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22322 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_simple_model(vocab_size, max_length=50):\n",
    "    \"\"\"Build a simplified text classification model that should work on any environment\"\"\"\n",
    "    # Ensure vocab_size is valid\n",
    "    vocab_size = max(3, vocab_size)  # Minimum of 3 for PAD, OOV and at least one word\n",
    "    print(f\"Creating model with vocab_size: {vocab_size}\")\n",
    "    \n",
    "    # Create a simpler model with fewer layers - the key change here is using Sequential with add()\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add embedding layer (with simpler config)\n",
    "    model.add(keras.layers.Embedding(input_dim=vocab_size, \n",
    "                              output_dim=16, \n",
    "                              input_length=max_length))\n",
    "    \n",
    "    # Flatten instead of GlobalAveragePooling (more stable)\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    # Single hidden layer\n",
    "    model.add(keras.layers.Dense(8, activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Use legacy optimizer for better compatibility\n",
    "    from tensorflow.keras.optimizers.legacy import Adam\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Print tokenizer information for debugging\n",
    "print(f\"Tokenizer num_words: {tokenizer.num_words}\")\n",
    "print(f\"Word index size: {len(tokenizer.word_index)}\")\n",
    "if tokenizer.word_index:\n",
    "    print(f\"First few words in vocab: {list(tokenizer.word_index.items())[:5]}\")\n",
    "\n",
    "# Build a simplified model\n",
    "vocab_size = max(tokenizer.num_words, len(tokenizer.word_index) + 1)  # Add 1 for padding token\n",
    "model = build_simple_model(vocab_size=vocab_size, max_length=max_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (600, 11), dtype: int32\n",
      "y_train shape: (600,), dtype: float32\n",
      "Starting model training with simple configuration...\n",
      "Epoch 1/5\n",
      "Error during training: Graph execution error:\n",
      "\n",
      "Detected at node StatefulPartitionedCall defined at (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "\n",
      "  File \"/tmp/ipykernel_49172/3385362601.py\", line 25, in <module>\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n",
      "\n",
      "DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_1345]\n",
      "\n",
      "Trying an even simpler training configuration...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743119383.492398   49833 service.cc:148] XLA service 0x7b5228008220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743119383.492424   49833 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-03-27 16:49:43.516211: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "E0000 00:00:1743119383.608136   49833 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "E0000 00:00:1743119383.631552   49833 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2025-03-27 16:49:43.635206: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "2025-03-27 16:49:43.635248: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback training also failed: Graph execution error:\n",
      "\n",
      "Detected at node StatefulPartitionedCall defined at (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "\n",
      "  File \"/tmp/ipykernel_49172/3385362601.py\", line 40, in <module>\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n",
      "\n",
      "  File \"/home/nlarion/Desktop/nlp_html_ads/nlp_html_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n",
      "\n",
      "DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_1863]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1743119384.040633   49833 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "E0000 00:00:1743119384.065117   49833 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2025-03-27 16:49:44.070509: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "2025-03-27 16:49:44.070581: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    }
   ],
   "source": [
    "# Ensure data is in numpy format (avoid TensorFlow tensor conversion issues)\n",
    "X_train_np = X_train.astype(np.int32)\n",
    "y_train_np = y_train.astype(np.float32)\n",
    "X_val_np = X_val.astype(np.int32)\n",
    "y_val_np = y_val.astype(np.float32)\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(f\"X_train shape: {X_train_np.shape}, dtype: {X_train_np.dtype}\")\n",
    "print(f\"y_train shape: {y_train_np.shape}, dtype: {y_train_np.dtype}\")\n",
    "\n",
    "# Define a simple callback that doesn't rely on external state\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Use a very small batch size and fewer epochs for stability\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "\n",
    "print(\"Starting model training with simple configuration...\")\n",
    "try:\n",
    "    # Train with a very simple configuration\n",
    "    history = model.fit(\n",
    "        X_train_np, y_train_np,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val_np, y_val_np),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    print(\"\\nTrying an even simpler training configuration...\")\n",
    "    \n",
    "    # Try training without validation data or callbacks\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_train_np, y_train_np,\n",
    "            epochs=3,\n",
    "            batch_size=4,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"Fallback training completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fallback training also failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Ads', 'Ads']))\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Not Ads', 'Ads'])\n",
    "plt.yticks([0, 1], ['Not Ads', 'Ads'])\n",
    "\n",
    "# Add labels to the plot\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model on New Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, tokenizer, text, max_length):\n",
    "    \"\"\"Predict class for a single text\"\"\"\n",
    "    # Tokenize text\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=max_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=tokenizer.word_index[tokenizer.pad_token]\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    \n",
    "    # Return result\n",
    "    is_ad = prediction > 0.5\n",
    "    label = 'Ad' if is_ad else 'Not Ad'\n",
    "    confidence = prediction if is_ad else 1 - prediction\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'label': label,\n",
    "        'confidence': float(confidence),\n",
    "        'raw_prediction': float(prediction)\n",
    "    }\n",
    "\n",
    "# Test on some new examples\n",
    "test_texts = [\n",
    "    \"Buy our new shoes with 30% discount today!\",\n",
    "    \"The weather is nice today, I'm going for a walk\",\n",
    "    \"Limited offer on all electronics this weekend\",\n",
    "    \"I watched a great movie yesterday with my friends\",\n",
    "    \"This is the best price you will find anywhere\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_text(model, tokenizer, text, max_length)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model for TensorFlow.js (with troubleshooting guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the tensorflowjs package installed\n",
    "# !pip install tensorflowjs\n",
    "\n",
    "# Save the model in Keras format first\n",
    "model_save_path = \"ad_classification_model\"\n",
    "model.save(model_save_path)\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "# This will create a model.json file and binary weight files\n",
    "tfjs_dir = \"tfjs_model\"\n",
    "!mkdir -p {tfjs_dir}\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, tfjs_dir)\n",
    "\n",
    "print(f\"Model saved in TensorFlow.js format at {tfjs_dir}\")\n",
    "\n",
    "# Save the vocabulary as well\n",
    "tokenizer.save_vocabulary(f\"{tfjs_dir}/vocabulary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example JavaScript Code to Use the Model in Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tfjs_model/example_usage.js\n",
    "// Example JavaScript code to use the model in a browser\n",
    "// Note: This is just an example and would need to be adapted to your specific use case\n",
    "\n",
    "// Load the model\n",
    "async function loadModel() {\n",
    "  const model = await tf.loadLayersModel('model.json');\n",
    "  return model;\n",
    "}\n",
    "\n",
    "// Load the vocabulary\n",
    "async function loadVocabulary() {\n",
    "  const response = await fetch('vocabulary.json');\n",
    "  const vocab = await response.json();\n",
    "  return vocab;\n",
    "}\n",
    "\n",
    "// Tokenize and pad text\n",
    "function tokenize(text, vocab, maxLength) {\n",
    "  // Convert to lowercase and remove punctuation\n",
    "  const cleanText = text.toLowerCase().replace(/[^\\w\\s]/g, '');\n",
    "  \n",
    "  // Split into words\n",
    "  const words = cleanText.split(/\\s+/);\n",
    "  \n",
    "  // Convert words to indices\n",
    "  const oovIndex = vocab['<OOV>'] || 1;\n",
    "  const sequence = words.map(word => vocab[word] || oovIndex);\n",
    "  \n",
    "  // Truncate or pad as needed\n",
    "  const padIndex = vocab['<PAD>'] || 0;\n",
    "  \n",
    "  if (sequence.length > maxLength) {\n",
    "    return sequence.slice(0, maxLength);\n",
    "  } else {\n",
    "    const padding = Array(maxLength - sequence.length).fill(padIndex);\n",
    "    return [...sequence, ...padding];\n",
    "  }\n",
    "}\n",
    "\n",
    "// Predict class for text\n",
    "async function predictText(text) {\n",
    "  // Load model and vocabulary\n",
    "  const [model, vocab] = await Promise.all([loadModel(), loadVocabulary()]);\n",
    "  \n",
    "  // Define max length\n",
    "  const maxLength = 50;\n",
    "  \n",
    "  // Tokenize text\n",
    "  const sequence = tokenize(text, vocab, maxLength);\n",
    "  \n",
    "  // Create tensor\n",
    "  const inputTensor = tf.tensor2d([sequence], [1, maxLength]);\n",
    "  \n",
    "  // Make prediction\n",
    "  const outputTensor = model.predict(inputTensor);\n",
    "  const prediction = await outputTensor.data();\n",
    "  \n",
    "  // Cleanup\n",
    "  inputTensor.dispose();\n",
    "  outputTensor.dispose();\n",
    "  \n",
    "  // Return result\n",
    "  const isAd = prediction[0] > 0.5;\n",
    "  const label = isAd ? 'Ad' : 'Not Ad';\n",
    "  const confidence = isAd ? prediction[0] : 1 - prediction[0];\n",
    "  \n",
    "  return {\n",
    "    text,\n",
    "    label,\n",
    "    confidence,\n",
    "    rawPrediction: prediction[0]\n",
    "  };\n",
    "}\n",
    "\n",
    "// Example usage\n",
    "async function main() {\n",
    "  const texts = [\n",
    "    \"Buy our new shoes with 30% discount today!\",\n",
    "    \"The weather is nice today, I'm going for a walk\"\n",
    "  ];\n",
    "  \n",
    "  for (const text of texts) {\n",
    "    const result = await predictText(text);\n",
    "    console.log(`Text: ${result.text}`);\n",
    "    console.log(`Prediction: ${result.label} (confidence: ${result.confidence.toFixed(4)})`);\n",
    "    console.log('---');\n",
    "  }\n",
    "}\n",
    "\n",
    "// Call main when document is loaded\n",
    "document.addEventListener('DOMContentLoaded', main);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Created a custom tokenizer that can load vocabulary from a JSON file\n",
    "2. Loaded a corpus from a JSON file (or created a sample one)\n",
    "3. Preprocessed the text data for NLP classification\n",
    "4. Built and trained a TensorFlow model to classify texts as \"ads\" or \"not ads\"\n",
    "5. Evaluated the model's performance\n",
    "6. Saved the model in a format compatible with TensorFlow.js\n",
    "7. Provided example JavaScript code to use the model in a browser\n",
    "\n",
    "The key components that satisfy the requirements are:\n",
    "- Custom tokenizer with JSON vocabulary loading (Section 2)\n",
    "- Corpus loading from JSON (Section 3)\n",
    "- Binary classification for \"ads\" and \"not ads\" (Sections 5-7)\n",
    "- Model export for TensorFlow.js (Section 9)\n",
    "\n",
    "For a real-world application, you may want to consider:\n",
    "- Using a more sophisticated model architecture (e.g., LSTM, Transformer)\n",
    "- Using pre-trained word embeddings\n",
    "- Implementing more advanced text preprocessing\n",
    "- Collecting a larger and more diverse dataset\n",
    "- Adding more classes if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
